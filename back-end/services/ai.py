"""
ServiÃ§o de IA (Groq + Llama 3)
IntegraÃ§Ã£o com o modelo de linguagem para o JuniBox
"""
from groq import Groq
from app_config import GROQ_API_KEY, MODEL_NAME, TEMPERATURE
from config.prompts import get_system_prompt
from typing import List, Dict, Any
from schemas import Message

# Inicializa o cliente Groq
client = None
if GROQ_API_KEY:
    try:
        client = Groq(api_key=GROQ_API_KEY)
    except Exception as e:
        print(f"âš ï¸  Erro ao inicializar Groq: {e}")

# ============================================
# FUNÃ‡ÃƒO SIMPLIFICADA PARA CHAT BÃSICO
# ============================================

def get_junibox_response(user_message: str, history: List[Message]) -> str:
    """
    FunÃ§Ã£o simplificada que monta o contexto e chama a Groq.
    VersÃ£o bÃ¡sica que nÃ£o precisa de Firebase - ideal para testes rÃ¡pidos.
    
    Args:
        user_message: Mensagem atual do usuÃ¡rio
        history: Lista de mensagens anteriores (Pydantic Message objects)
        
    Returns:
        Resposta gerada pelo JuniBox
    """
    if not client:
        return "âš ï¸ ServiÃ§o de IA nÃ£o estÃ¡ configurado. Verifique a GROQ_API_KEY."
    
    if not user_message or not user_message.strip():
        return "Por favor, envie uma mensagem vÃ¡lida."
    
    # 1. ComeÃ§a com o System Prompt (A personalidade do JuniBox)
    system_prompt = get_system_prompt()
    messages_payload = [{"role": "system", "content": system_prompt}]
    
    # 2. Adiciona o histÃ³rico antigo (convertendo do Pydantic para dict)
    for msg in history:
        messages_payload.append({"role": msg.role, "content": msg.content})
    
    # 3. Adiciona a mensagem atual do usuÃ¡rio
    messages_payload.append({"role": "user", "content": user_message})
    
    # 4. Chama a API do Groq
    try:
        chat_completion = client.chat.completions.create(
            messages=messages_payload,
            model=MODEL_NAME,
            temperature=TEMPERATURE,  # Baixa criatividade para seguir regras
            max_tokens=1024
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        print(f"âŒ Erro ao processar na Groq: {e}")
        return f"Erro ao processar na Groq: {str(e)}"

# ============================================
# FUNÃ‡Ã•ES AVANÃ‡ADAS (COM FIREBASE)
# ============================================

def generate_junibox_response(
    message: str, 
    history: List[Dict[str, str]], 
    idea_context: Dict[str, Any]
) -> str:
    """
    Gera uma resposta do JuniBox baseada no contexto da ideia e histÃ³rico
    
    Args:
        message: Mensagem atual do usuÃ¡rio
        history: HistÃ³rico de mensagens anteriores [{"role": "user/assistant", "content": "..."}]
        idea_context: Dados atuais da ideia (tÃ­tulo, descriÃ§Ã£o, etc)
        
    Returns:
        Resposta gerada pela IA
    """
    if not client:
        return "âš ï¸ ServiÃ§o de IA nÃ£o estÃ¡ configurado. Verifique a GROQ_API_KEY."
    
    # Injeta o contexto da ideia no prompt do sistema
    context_str = _build_context_string(idea_context)
    
    # Monta a lista de mensagens para a API
    system_prompt = get_system_prompt()
    messages = [
        {"role": "system", "content": system_prompt + "\n\n" + context_str}
    ]
    
    # Adiciona histÃ³rico de conversas anteriores
    messages.extend(history)
    
    # Adiciona a mensagem atual do usuÃ¡rio
    messages.append({"role": "user", "content": message})
    
    try:
        # Chama a API do Groq
        completion = client.chat.completions.create(
            messages=messages,
            model=MODEL_NAME,
            temperature=TEMPERATURE,
            max_tokens=1024,
            top_p=1,
            stream=False
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        print(f"âŒ Erro ao gerar resposta da IA: {e}")
        return "Desculpe, tive um problema ao processar sua mensagem. Tente novamente em alguns instantes."

def _build_context_string(idea_context: Dict[str, Any]) -> str:
    """
    ConstrÃ³i uma string formatada com o contexto da ideia
    para injetar no prompt do sistema
    
    Args:
        idea_context: DicionÃ¡rio com os dados da ideia
        
    Returns:
        String formatada com o contexto
    """
    if not idea_context:
        return "DADOS ATUAIS DA IDEIA: Ainda nÃ£o hÃ¡ informaÃ§Ãµes sobre a ideia."
    
    context_parts = ["DADOS ATUAIS DA IDEIA DO USUÃRIO:"]
    
    # Campos principais
    if idea_context.get('title'):
        context_parts.append(f"ðŸ“Œ TÃ­tulo: {idea_context['title']}")
    
    if idea_context.get('description'):
        context_parts.append(f"ðŸ“ DescriÃ§Ã£o: {idea_context['description']}")
    
    if idea_context.get('target_audience'):
        context_parts.append(f"ðŸ‘¥ PÃºblico-alvo: {idea_context['target_audience']}")
    
    if idea_context.get('status'):
        context_parts.append(f"ðŸ“Š Status: {idea_context['status']}")
    
    # Campos dinÃ¢micos adicionais
    if idea_context.get('dynamic_content'):
        dynamic = idea_context['dynamic_content']
        if dynamic:
            context_parts.append("\nðŸ”§ Campos Adicionais:")
            for key, value in dynamic.items():
                context_parts.append(f"  â€¢ {key}: {value}")
    
    return "\n".join(context_parts)

def generate_idea_suggestions(idea_context: Dict[str, Any]) -> List[str]:
    """
    Gera sugestÃµes automÃ¡ticas para melhorar a ideia
    Pode ser usado para o painel de sugestÃµes da UI
    
    Args:
        idea_context: Dados da ideia
        
    Returns:
        Lista de sugestÃµes
    """
    if not client:
        return ["Configure a GROQ_API_KEY para receber sugestÃµes."]
    
    context_str = _build_context_string(idea_context)
    
    prompt = f"""
{context_str}

Com base nos dados acima, gere 3 sugestÃµes objetivas e acionÃ¡veis para melhorar esta ideia.
Cada sugestÃ£o deve ter no mÃ¡ximo 2 linhas.
Retorne apenas as sugestÃµes, uma por linha, sem numeraÃ§Ã£o.
"""
    
    try:
        system_prompt = get_system_prompt()
        completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            model=MODEL_NAME,
            temperature=0.5,
            max_tokens=300
        )
        
        response = completion.choices[0].message.content
        suggestions = [s.strip() for s in response.split('\n') if s.strip()]
        
        return suggestions[:3]  # Garante no mÃ¡ximo 3 sugestÃµes
        
    except Exception as e:
        print(f"âŒ Erro ao gerar sugestÃµes: {e}")
        return ["NÃ£o foi possÃ­vel gerar sugestÃµes no momento."]

def validate_idea_completeness(idea_context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Analisa se a ideia estÃ¡ completa e pronta para submissÃ£o
    
    Args:
        idea_context: Dados da ideia
        
    Returns:
        DicionÃ¡rio com score de completude e campos faltantes
    """
    required_fields = ['title', 'description', 'target_audience']
    missing_fields = []
    filled_fields = 0
    
    for field in required_fields:
        value = idea_context.get(field, "")
        if value and len(str(value).strip()) > 0:
            filled_fields += 1
        else:
            missing_fields.append(field)
    
    completeness_score = (filled_fields / len(required_fields)) * 100
    
    return {
        "score": completeness_score,
        "is_complete": completeness_score == 100,
        "missing_fields": missing_fields,
        "filled_fields": filled_fields,
        "total_fields": len(required_fields)
    }

